{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Natural Language API Client\n",
    "## Setup\n",
    "[https://cloud.google.com/natural-language/docs/reference/libraries#client-libraries-install-python](https://cloud.google.com/natural-language/docs/reference/libraries#client-libraries-install-python)\n",
    "\n",
    "Run:\n",
    "```\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=\"[PATH]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Code\n",
    "Run the below code to make sure you have everything set up. Something should show up similar to:\n",
    "```\n",
    "Text: Hello, world!\n",
    "Sentiment: 0.300000011921, 0.300000011921\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "# The text to analyze\n",
    "text = u'Hello, world!'\n",
    "document = types.Document(\n",
    "    content=text,\n",
    "    type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "# Detects the sentiment of the text\n",
    "sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "\n",
    "print('Text: {}'.format(text))\n",
    "print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST API Call\n",
    "\n",
    "Export API Key to environmental variable:\n",
    "```\n",
    "export GOOGLE_KEY=MY_SECRET_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "META_CELLPHONE = 'Datasets/meta_Cell_Phones_and_Accessories.json.gz'\n",
    "\n",
    "def get_documents(file_name):\n",
    "    g = gzip.open(file_name, 'r')\n",
    "    results = []\n",
    "    for line in g:\n",
    "        document = eval(line)\n",
    "        results.append(document)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_documents = get_documents(META_CELLPHONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ['GOOGLE_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "LANGUAGE_ENDPOINT = 'https://language.googleapis.com/v1/documents:analyzeEntities?key={}'.format(api_key)\n",
    "\n",
    "def merge_title_description(document):\n",
    "    title = ''\n",
    "    description = ''\n",
    "    if 'title' in document:\n",
    "        title = document['title']\n",
    "    if 'description' in document:\n",
    "        description = document['description']\n",
    "    return title + description\n",
    "\n",
    "def perform_api_request(document):\n",
    "    payload = {\n",
    "        \"document\": {\n",
    "            \"type\": \"PLAIN_TEXT\",\n",
    "            \"language\": \"EN\",\n",
    "            \"content\": merge_title_description(document)\n",
    "        },\n",
    "        \"encodingType\": \"UTF8\"\n",
    "    }\n",
    "\n",
    "    r = requests.post(LANGUAGE_ENDPOINT, data=json.dumps(payload))\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_api_request(document_name, r):\n",
    "    file_name = 'Processed/{}.json'.format(document_name)\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json_data = json.dump(r, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serialize_processed_document(asin):\n",
    "    already_processed = os.path.exists('Processed/{}.json'.format(asin))\n",
    "    if already_processed:\n",
    "        data = json.load(open('Processed/{}.json'.format(asin), 'r'))\n",
    "        return json.loads(data)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "RUN_LIMIT = 5000\n",
    "def process_all_documents(all_documents):\n",
    "    count = 0\n",
    "    for document in all_documents:\n",
    "        asin = document['asin']\n",
    "        already_processed = os.path.exists('Processed/{}.json'.format(asin))\n",
    "        serialized = serialize_processed_document(asin)\n",
    "        \n",
    "        if not already_processed:\n",
    "            r = perform_api_request(document)\n",
    "            save_api_request(asin, r)\n",
    "            count = count + 1\n",
    "            if (count == RUN_LIMIT):\n",
    "                break\n",
    "\n",
    "process_all_documents(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files_into_memory(all_documents):\n",
    "    serialized_documents = {}\n",
    "    for document in all_documents:\n",
    "        asin = document['asin']\n",
    "        already_processed = os.path.exists('Processed/{}.json'.format(asin))\n",
    "        if already_processed:\n",
    "            serialized = serialize_processed_document(asin)\n",
    "            serialized_documents[asin] = serialized\n",
    "        else:\n",
    "            return serialized_documents        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serialized = read_files_into_memory(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B002M25U0E\n",
      "B003ZYHOK2\n",
      "B002ES6F3M\n",
      "B0038YGUM2\n",
      "B0035VECNM\n",
      "B0012UIDRO\n",
      "B001656W46\n",
      "B002P4YXBQ\n",
      "B003XP09A0\n",
      "B001RD858I\n"
     ]
    }
   ],
   "source": [
    "def read_first_ten(serialized):\n",
    "    count = 0\n",
    "    for document in serialized:\n",
    "        print document\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "read_first_ten(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we are just working with the highest salience CONSUMER_GOODS. Later, we will make it more complex and consider all CONSUMER_GOODS returned from each product. We will also be erasing duplicates that come up.\n",
    "\n",
    "temporary code here that will remove capitilization and spaces. We will eventually need a classifier to return us the same string that will be used as the key to our map.\n",
    "\n",
    "        entities = {}\n",
    "        name = \"\"\n",
    "        # function to parse file \n",
    "         for entity in entities.entities:\n",
    "             # The entities are sorted by highest salience -> lowest salience\n",
    "             if (entity.type == language.types.Entity.CONSUMER_GOOD):\n",
    "                 # for now, just remove capitilization and spaces. but classifier is ideal\n",
    "                 name = ''.join(entity.name.split()).lower()\n",
    "                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getConsumerGood(file):\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I'm assuming that the map<asin, file>files exists and the function getConsumerGood\n",
    "# exists that takes in a file and returns the highest salience consumer good name \n",
    "all_categories = {}\n",
    "def map_categories(all_documents):\n",
    "    for document in all_documents:\n",
    "        name = getConsumerGood(files[document.asin])\n",
    "        for category in document['categories']:\n",
    "            if (category[0][0] in all_categories):\n",
    "                sub_category = all_categories[category[0][0]] # map<category, map<consumer_good, list<document> > >\n",
    "                if(category[0][1] in sub_category):\n",
    "                    if(name in sub_category[category[0][1]]):\n",
    "                        sub_category[category[0][1]][name].append(document[i]) \n",
    "                    else:\n",
    "                        sub_category[category[0][1]][name] = [document[i]]\n",
    "                else: \n",
    "                    sub_category[category[0][1]] = {name: [document[i]]}\n",
    "            else:\n",
    "                all_categories[category[0][0]] = {category[0][1]: {name: [document[i]]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query-based Algorithm\n",
    "This will recommend three items from each sub-category that the product isn't classified as in the same overall category based on the query of the user. The query will come in a json file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map_categories(all_documents)\n",
    "def recommendItems(query):\n",
    "    recommend = []\n",
    "    name = getConsumerGood(query)\n",
    "    category = query['categories'][0][0]\n",
    "    sub_category = query['categories'][0][1]\n",
    "    for key in all_categories[category].keys():\n",
    "        if (key == sub_category):\n",
    "            continue\n",
    "        if (not all_categories[category][key][name]): #empty\n",
    "            continue\n",
    "        for i in range(1,3):\n",
    "            recommend.append(all_categories[category][key][name][i])\n",
    "    return recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recommendItems(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
